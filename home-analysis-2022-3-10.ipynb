{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f32d6-8780-4f32-ab66-6e869149d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# Turn this on for higher DPI on images\n",
    "# mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a20450-fa82-4b05-a924-9c952457f743",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "After downloading `dolt`, I exported the data to CSV (though you could read directly from the databse with `polars` or `pandas`)\n",
    "```\n",
    "dolt table export sales sales-2022-03-10.csv\n",
    "```\n",
    "Then I did\n",
    "```\n",
    "grep -ax '.*' sales-2022-03-10.csv > sales-2022-03-10-cleaned.csv  \n",
    "```\n",
    "to get rid of Unicode errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee1f57-3153-4228-b9c9-f1038c2b3aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'property_id': str,\n",
    "    'zip5': str,\n",
    "}\n",
    "\n",
    "\n",
    "lazydf = pl.scan_csv('sales-2022-03-10-cleaned.csv', dtypes = dtypes, encoding = 'utf8-lossy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996adbe-20d4-4c7d-93ac-c25fe37c6406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outlier_index() -> pl.Expr:\n",
    "    \"\"\"\n",
    "    We want to create an index for each sale to know if it's an outlier.\n",
    "    This is the ratio of the highest price to the lowest price.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (pl.col('sale_price').sort().last()/pl.col('sale_price').sort().first()).over('property_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20212e63-5aef-47ff-b850-32e6a496680f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter main dataframe down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c1603-c268-43c3-b0b9-d18401fc8318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_property_types = 'dwelling|home|condo|family|residence|residential|apartment|homesite|duplex|triplex|fourplex|resid|apt'\n",
    "\n",
    "df = (\n",
    "    lazydf\n",
    "    .filter(pl.col('property_type').str.to_lowercase().str.contains(selected_property_types))\n",
    "    .filter(pl.col('sale_price') > 20_000)\n",
    "    \n",
    "    # remove weird address\n",
    "    .filter(~pl.col('physical_address').str.contains('^0 |^- '))\n",
    "    .filter(pl.col('physical_address').str.lengths() > 5)\n",
    "    \n",
    "    .with_column(\n",
    "        pl.col('sale_date').str.strptime(pl.Datetime, fmt = \"%Y-%m-%d %H:%M:%S %z %Z\", strict = False)\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col('state').cast(pl.Categorical),\n",
    "            pl.col('sale_date').dt.year().alias('sale_year'),\n",
    "            pl.col('sale_date').dt.month().alias('sale_month'),            \n",
    "            pl.col('sale_date').dt.day().alias('sale_day'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # sanity check -- some houses have weird sale years\n",
    "    .filter(pl.col('sale_year') < 2022)\n",
    "    \n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col('sale_year') - pl.col('year_built')).alias('property_age'),\n",
    "            outlier_index().alias('outlier_index'),\n",
    "            pl.lit(1).count().over('physical_address').alias('property_count'),\n",
    "            pl.lit(1).count().over(['property_id', 'physical_address']).alias('times_sold'),   \n",
    "            pl.col('sale_price').max().over('property_id').alias('sale_max'),\n",
    "            pl.col('sale_price').min().over('property_id').alias('sale_min'),\n",
    "            pl.col('sale_price').median().over(['physical_address', 'property_id']).alias('sale_median'),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.max([pl.col('sale_price')/pl.col('sale_median'),pl.col('sale_median')/pl.col('sale_price')]).alias('outlier_index')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # not needed for this analysis\n",
    "    .drop(['source_url', 'book', 'page'])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1e276-c8ae-4423-87c8-cfa235fdec21",
   "metadata": {},
   "source": [
    "## `predata` is a dataframe which holds some prefiltered data\n",
    "\n",
    "After loading the entire dataframe into `df`, I wanted slightly more control for testing and plotting purposes. So I made a pre-filtered dataframe called `predata`, big enough that I could do the whole analysis with it, but not so big that the plots took forever to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335dba9-9eb3-4525-a56e-d9fec824c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predata = (\n",
    "    # comment out the sample if you want the complete data\n",
    "    df.sample(frac = .01)\n",
    "    \n",
    "    # we don't have complete records for these throughout the last decade\n",
    "    # this skews the relationship of sale_date to sale\n",
    "    .filter(pl.col('state') != 'WI')\n",
    "    .filter(pl.col('state') != 'NC')\n",
    "    \n",
    "    # remove any properties that appear more than 100 times as the same address\n",
    "    .filter(pl.col('property_count') < 100)\n",
    "    \n",
    "    # ...with \"age\" < 0\n",
    "    .filter(pl.col('property_age') >= 0)\n",
    "    \n",
    "    # ...where the min/max scores differ by too much\n",
    "    .filter(pl.col('outlier_index') < 3)\n",
    "    \n",
    "    # ...where the sale_year is too old\n",
    "    .filter(pl.col('sale_year') > 1975)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f531de-c325-4280-9ef9-f7659b206c50",
   "metadata": {},
   "source": [
    "### Scatterplot of all sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e25eb-60cd-4a17-b841-ff15e541573a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "data = predata.clone()\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .01, alpha = .01)\n",
    "\n",
    "yticks = {0:         '$0',\n",
    "          100_000:   '$100k',\n",
    "          200_000:   '$200k',\n",
    "          500_000:   '$500k',\n",
    "          1_000_000: '$1M',\n",
    "          1_500_000: '$1.5M',\n",
    "          2_000_000: '$2M',}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title('Residential property sales in the US')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(16)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 13)\n",
    "ax.yaxis.set_tick_params(size = 13)\n",
    "\n",
    "plt.savefig('images/sales_total.png', facecolor = 'white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b5184-76c3-4d7f-b841-991a599f5d10",
   "metadata": {},
   "source": [
    "### New-house premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b53dea-621f-4ba7-89a7-f199508ce20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_age = 25\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter(pl.col('property_age') <= max_age)\n",
    "    .with_column(\n",
    "         pl.col('sale_price').mean().over('sale_year').alias('mean_price_per_year')\n",
    "     )\n",
    "    .filter(pl.col('sale_price') < pl.col('mean_price_per_year')*40)\n",
    "    .groupby(['property_age', 'sale_year'])\n",
    "    .agg([pl.col('sale_price').mean(), pl.col('mean_price_per_year').first().alias('ppy')])\n",
    "    .with_column(\n",
    "         (pl.col('sale_price_mean')/pl.col('ppy')).alias('ratio')\n",
    "     )[['property_age', 'ratio']]\n",
    "    .groupby('property_age')      \n",
    "    .agg(pl.col('ratio').mean())      \n",
    "    .sort('property_age')\n",
    ")\n",
    "\n",
    "xs, ys = data\n",
    "\n",
    "ax.set_title('The new-house premium: how do house prices depreciate with age?')\n",
    "ax.set_ylabel('premium (%)')\n",
    "ax.set_xlabel('property age')\n",
    "\n",
    "ax.set_ylim([.6, 1.4])\n",
    "ax.set_xlim([0, max_age])\n",
    "\n",
    "ax.title.set_size(16)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 13)\n",
    "ax.yaxis.set_tick_params(size = 13)\n",
    "\n",
    "ax.step(np.asarray(xs), np.asarray(ys))\n",
    "\n",
    "plt.savefig('images/new_house_premium.png', facecolor = 'white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee2992-5ed9-4708-b18d-89c861bb0b42",
   "metadata": {},
   "source": [
    "### Finding appreciation rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfcb34-fa55-4c5f-b5e1-151a89b221aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 750_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_median') < max_median)\n",
    "    # .filter(pl.col('sale_median') > 25_000)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .2,  alpha = .02)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [350, .001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "ax.plot([np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt],\n",
    "        [1000*func(x-730_000, *popt) for x in xs_plt], \n",
    "        color = 'red', \n",
    "        label = f'appreciation approx. {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "yticks = {100_000:   '$100k',\n",
    "          250_000:   '$250k',\n",
    "          500_000:   '$500k',\n",
    "          1_000_000: '$1M',}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, median_sale < ${max_median}k')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "\n",
    "plt.savefig('images/appreciation_total_less_than_750k.png', facecolor = 'white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3e108-cdbf-4570-894e-6b8149cf0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 750_000\n",
    "min_median = 250_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_median') < max_median)\n",
    "    .filter(pl.col('sale_median') > min_median)\n",
    "    .filter(pl.col('times_sold') > 1)\n",
    "    # .filter(pl.col('sale_median') > 25_000)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .05,  alpha = .1)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [350, .001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "ax.plot([np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt],\n",
    "        [1000*func(x-730_000, *popt) for x in xs_plt], \n",
    "        color = 'red', \n",
    "        label = f'appreciation approx. {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "yticks = {100_000:   '$100k',\n",
    "          250_000:   '$250k',\n",
    "          500_000:   '$500k',\n",
    "          1_000_000: '$1M',}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, \\$250k < median_sale < \\$750k')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "\n",
    "plt.savefig('images/appreciation_total_middle_median.png', facecolor = 'white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ce2ca-eb56-46c7-af84-220d7d6c43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 1_500_000\n",
    "min_median = 750_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_median') < max_median)\n",
    "    .filter(pl.col('sale_median') > min_median)\n",
    "    .filter(pl.col('times_sold') > 1)\n",
    "    # .filter(pl.col('sale_median') > 25_000)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .2,  alpha = .2)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [350, .001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "ax.plot([np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt],\n",
    "        [1000*func(x-730_000, *popt) for x in xs_plt], \n",
    "        color = 'red', \n",
    "        label = f'Prices increase at about {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "yticks = {\n",
    "    0: '$0',\n",
    "    500_000:   '$500k',\n",
    "    1_000_000: '$1M',\n",
    "    2_000_000: '$2M',\n",
    "    3_000_000: '$3M',\n",
    "}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, \\$750k < median_sale < \\$1.5M')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "\n",
    "plt.savefig('images/appreciation_total_high_median.png', facecolor = 'white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb57af8-a328-4cc3-b5c8-cafffd70c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 3_000_000\n",
    "min_median = 1_500_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_median') < max_median)\n",
    "    .filter(pl.col('sale_median') > min_median)\n",
    "    .filter(pl.col('times_sold') > 1)\n",
    "    # .filter(pl.col('sale_median') > 25_000)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .2,  alpha = .5)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [350, .001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "ax.plot([np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt],\n",
    "        [1000*func(x-730_000, *popt) for x in xs_plt], \n",
    "        color = 'red', \n",
    "        label = f'Prices increase at about {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "yticks = {\n",
    "    0: '$0',\n",
    "    1_000_000: '$1M',\n",
    "    3_000_000: '$3M',\n",
    "    5_000_000: '$5M',\n",
    "}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, \\$1.5M < median_sale < \\$3M')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "\n",
    "plt.savefig('images/appreciation_total_very_high_median.png', facecolor = 'white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ae65a-4ed8-460e-a262-41eb37e5924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 400_000\n",
    "min_median = 100_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_median') < max_median)\n",
    "    .filter(pl.col('sale_median') > min_median)\n",
    "    .filter(pl.col('times_sold') > 1)\n",
    "    # .filter(pl.col('sale_median') > 25_000)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .05,  alpha = .1)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [350, .001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "ax.plot([np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt],\n",
    "        [1000*func(x-730_000, *popt) for x in xs_plt], \n",
    "        color = 'red', \n",
    "        label = f'Prices increase at about {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "yticks = {\n",
    "    0: '$0',\n",
    "    100_000: '$100k',\n",
    "    300_000: '$300k',\n",
    "    500_000: '$500k',\n",
    "    700_000: '$600k',\n",
    "}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, \\$100k < median_sale < \\$400k')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "plt.savefig('images/appreciation_total_low_median.png', facecolor = 'white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda1900-2509-46bc-b98b-0cfe4b73f507",
   "metadata": {},
   "source": [
    "### Cheapest month to buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523254d7-36b0-43bd-b7f3-940587def78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_median') < max_median)\n",
    ")\n",
    "\n",
    "start_date = datetime(2011, 1, 1, 0, 0)\n",
    "\n",
    "data['adjusted_sale_price'] = pl.Series(data.apply(lambda row: row[8]*np.exp((start_date - row[6]).days*popt[1]))['apply'])\n",
    "\n",
    "xs, ys = (\n",
    "    data\n",
    "    .groupby('sale_month')\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col('adjusted_sale_price').mean()\n",
    "        ]\n",
    "    )\n",
    "    .sort('sale_month')\n",
    ")\n",
    "\n",
    "ax.bar(xs, ys)\n",
    "\n",
    "yticks = {val: f'${val//1_000}k' for val in range(220_000, 280_000, 10_000)}\n",
    "\n",
    "plt.title(\"What's the cheapest month to buy a house?\")\n",
    "ax.set_ylabel('adjusted sale price [USD]')\n",
    "ax.set_xlabel('sale month')\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_xlim([0, 13])\n",
    "ax.set_ylim([ys.min()/1.1, ys.max()*1.05])\n",
    "\n",
    "ax.set_xticks([i for i in range(1,12+1)])\n",
    "ax.set_xticklabels([calendar.month_abbr[i] for i in ax.get_xticks()])\n",
    "ax.xaxis.set_tick_params(rotation = 10)\n",
    "\n",
    "ax.title.set_size(20)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "plt.savefig('images/cheapest_month_to_buy.png', facecolor = 'white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9db522-554a-493b-817a-867db85fd95d",
   "metadata": {},
   "source": [
    "### Sales counts over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c73cd-d47e-4ffd-8948-ea4c2b81edd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16, 6))\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .groupby('sale_date')\n",
    "    .agg(\n",
    "        [\n",
    "            pl.lit(1).count()\n",
    "        ]\n",
    "    )\n",
    "    .sort('sale_date')\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "ys = np.asarray(data['literal_count'])\n",
    "\n",
    "def moving_average(a, n = 3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "window = 60\n",
    "xs = xs[window - 1:]\n",
    "ys = moving_average(ys, n = window)\n",
    "\n",
    "plt.title('Number of property sales over time (2 month moving average)')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('num. property sales [avg]')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([0, ys.max()*1.2])\n",
    "\n",
    "ax.title.set_size(16)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.plot(xs, ys)\n",
    "\n",
    "plt.savefig('images/property_sales_over_time.png', facecolor = 'white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2628c7-2557-49f8-aa93-1efbb41f1b70",
   "metadata": {},
   "source": [
    "### Sales counts over time by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a3bf9-cd8b-43d4-96b9-1a4c7331415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .groupby(['sale_date', 'state'])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.lit(1).count()\n",
    "        ]\n",
    "    )\n",
    "    .sort('sale_date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029d236-2a45-46f2-8e55-3a75e1aded15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16, 6))\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .groupby(['sale_date', 'state'])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.lit(1).count()\n",
    "        ]\n",
    "    )\n",
    "    .sort('sale_date')\n",
    ")\n",
    "\n",
    "states = data['state'].unique()\n",
    "\n",
    "def moving_average(a, n = 3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "for i, state in enumerate(states):\n",
    "\n",
    "    data2 = data.filter(pl.col('state') == state)\n",
    "    \n",
    "    window = 60\n",
    "\n",
    "    xs = [np.datetime64(t) for t in data2['sale_date']]\n",
    "    ys = np.asarray(data2['literal_count'])\n",
    "    \n",
    "    if i == 0:\n",
    "        globxmin = min(xs)\n",
    "        globxmax = max(xs)\n",
    "        \n",
    "    else: \n",
    "        if min(xs) < globxmin:\n",
    "            globxmin = min(xs)\n",
    "\n",
    "        if max(xs) > globxmax:\n",
    "            globxmax = max(xs)\n",
    "\n",
    "    xs = xs[window - 1:]\n",
    "    ys = moving_average(ys, n = window)\n",
    "\n",
    "    ax.plot(xs, ys, alpha = 1, label = state)\n",
    "\n",
    "plt.title('Property sales by state (2 month moving average)')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('num. property sales [avg]')\n",
    "\n",
    "ax.set_xlim([globxmin, globxmax])\n",
    "ax.set_ylim([0, 600])\n",
    "\n",
    "ax.title.set_size(16)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('images/property_sales_by_state.png', facecolor = 'white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ca869-8c1e-47f4-a411-dadb02b2fa9f",
   "metadata": {},
   "source": [
    "### Rurality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fadbd-0e5e-4870-a20d-1a71a3162481",
   "metadata": {},
   "outputs": [],
   "source": [
    "_fips_rurality = pl.read_csv('fips_rurality.csv', dtypes = {'FIPS': str})\n",
    "_zips_fips_conv = pl.read_csv('zip_fips_conversion.csv', dtypes = {'ZIP': str, 'STCOUNTYFP': str})\n",
    "zip_info = _zips_fips_conv.join(_fips_rurality, left_on = 'STCOUNTYFP', right_on = 'FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22d1ae-ff87-42cc-886a-c11aa2bfab57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "data = (\n",
    "    predata.filter(pl.col('sale_year') > 2000).filter(pl.col('sale_price') < 5_000_000)\n",
    ").join(zip_info[['ZIP', 'RUCC_2013']], left_on = 'zip5', right_on = 'ZIP')\n",
    "\n",
    "urban_score_split = 1\n",
    "\n",
    "data1 = data.filter(pl.col('RUCC_2013') > urban_score_split).groupby('sale_date').agg([pl.col('sale_price').mean()]).sort('sale_date')\n",
    "data2 = data.filter(pl.col('RUCC_2013') <= urban_score_split).groupby('sale_date').agg([pl.col('sale_price').mean()]).sort('sale_date')\n",
    "\n",
    "xs1 = [np.datetime64(t) for t in data1['sale_date']]\n",
    "ys1 = np.asarray(data1['sale_price_mean'])\n",
    "\n",
    "xs2 = [np.datetime64(t) for t in data2['sale_date']]\n",
    "ys2 = np.asarray(data2['sale_price_mean'])\n",
    "\n",
    "def moving_average(a, n = 3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "window = 120\n",
    "xs1 = xs1[window - 1:]\n",
    "ys1 = moving_average(ys1, n = window)\n",
    "\n",
    "xs2 = xs2[window - 1:]\n",
    "ys2 = moving_average(ys2, n = window)\n",
    "\n",
    "ax.step(xs1, ys1, c = 'r', label = 'more rural')\n",
    "ax.step(xs2, ys2, c = 'b', label = 'more urban')\n",
    "\n",
    "yticks = {100_000: '$100k',\n",
    "          250_000: '$250k',\n",
    "          500_000: '$500k',\n",
    "          750_000: '$750k',\n",
    "          1_000_000: '$1M',}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title('Sale prices by category, 2010-2020')\n",
    "ax.set_ylabel('mean sale price by category [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs2), max(xs2)])\n",
    "ax.set_ylim([0, 1_000_000])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 15)\n",
    "\n",
    "plt.savefig('images/sale_prices_by_category.png', facecolor = 'white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750395f-e303-4c41-a1ec-a32421ab08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "cmap = cm.autumn\n",
    "norm = Normalize(vmin=0, vmax=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b915e-864d-40cf-aaca-55a1f0c8ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "data = (\n",
    "    predata.filter(pl.col('sale_year') > 2000).filter(pl.col('sale_price') < 5_000_000)\n",
    ").join(zip_info[['ZIP', 'RUCC_2013']], left_on = 'zip5', right_on = 'ZIP')\n",
    "\n",
    "def moving_average(a, n = 3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "for i in range(0,7):\n",
    "    data1 = data.filter(pl.col('RUCC_2013') == i).groupby('sale_date').agg([pl.col('sale_price').mean()]).sort('sale_date')\n",
    "\n",
    "    xs1 = [np.datetime64(t) for t in data1['sale_date']]\n",
    "    ys1 = np.asarray(data1['sale_price_mean'])\n",
    "\n",
    "    window = 120\n",
    "    xs1 = xs1[window - 1:]\n",
    "    ys1 = moving_average(ys1, n = window)\n",
    "\n",
    "    ax.step(xs1, ys1, c= cmap(norm(i)), label = f'{i}')\n",
    "\n",
    "\n",
    "yticks = {100_000: '$100k',\n",
    "          250_000: '$250k',\n",
    "          500_000: '$500k',\n",
    "          750_000: '$750k',\n",
    "          1_000_000: '$1M',}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title('Sale prices by RUCC rurality score (0 is most urban, 6 is most rural), 2010-2020')\n",
    "ax.set_ylabel('mean sale price by category [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs1), max(xs1)])\n",
    "ax.set_ylim([0, 1_000_000])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 15)\n",
    "\n",
    "plt.savefig('images/sale_prices_by_category_detailed.png', facecolor = 'white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ba54c-705f-44e8-a2ff-69d05c811baa",
   "metadata": {},
   "source": [
    "### Finding the most common buyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493a43c-d3b9-44ae-98a1-45c87d212016",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyers = (df.\n",
    "    filter(pl.col('buyer_name') != '')\n",
    "    .with_column(pl.lit(1).alias('counts'))\n",
    "    .groupby(['buyer_name', 'sale_year'])\n",
    "    .agg(pl.col('counts').count())\n",
    "    .sort('counts_count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fae05-1abb-4364-990f-7042c3bb820b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for year in [2017, 2018, 2019, 2020, 2021]:\n",
    "    print(buyers.filter(pl.col('sale_year') == year).sort('counts_count').tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf322190-2bb9-4dad-817d-b985a7d6238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((buyers\n",
    " .filter(pl.col('sale_year') > 2015)\n",
    " .groupby(['sale_year', 'buyer_name'])\n",
    " .agg(pl.col('buyer_name').count())\n",
    " .groupby('sale_year')\n",
    " .apply(lambda x: x.sort('buyer_name_count', reverse = False).tail(3))    \n",
    " .sort('sale_year')\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd29753-80eb-4f0a-b731-919b9b0b745a",
   "metadata": {},
   "source": [
    "### ZIP Code map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dc1ff-2499-414f-863d-a28998eee6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "from vega_datasets import data as vegadata\n",
    "\n",
    "zipsource = pl.DataFrame(pd.read_csv(vegadata.zipcodes.url, dtype = {'zip_code': str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c3777-4950-4c56-90be-f344cc406ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = predata.join(zipsource, left_on = 'zip5', right_on = 'zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209880d-61f8-4d21-9d42-7bfb91d9ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = (data\n",
    "      .groupby(['zip5', 'latitude', 'longitude'])\n",
    "      .agg(pl.count())\n",
    "      .with_column(\n",
    "          np.log10(pl.col('count')).cast(int).alias('logct')\n",
    "      )\n",
    "      .to_pandas()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024a169-29fc-44fe-836c-1b8f88be4749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states = alt.topo_feature(vegadata.us_10m.url, feature='states')\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# US states background\n",
    "background = alt.Chart(states).mark_geoshape(\n",
    "    fill='white',\n",
    "    stroke='gray'\n",
    ").properties(\n",
    "    width=975,\n",
    "    height=600\n",
    ").project('albersUsa')\n",
    "\n",
    "points = (\n",
    "    alt.Chart(dt)\n",
    "    .mark_circle(size=15).encode(\n",
    "        longitude='longitude:Q',\n",
    "        latitude='latitude:Q',\n",
    "        color='logct:Q',\n",
    "        # size = 'logct:N',\n",
    "        tooltip='zip5:N'\n",
    "    ).project(\n",
    "        type='albersUsa'\n",
    "    ).properties(\n",
    "        width=975,\n",
    "        height=600\n",
    "    )\n",
    ")\n",
    "\n",
    "# points.save('zip_distribution.png')\n",
    "background + points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d7b90-d5d3-4200-a1f3-4d4ac152f5d2",
   "metadata": {},
   "source": [
    "### LA MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e02380-a54c-4a21-a6be-94a45e6799cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16782ed-15f8-46bb-9b8a-8a3849dbfdf5",
   "metadata": {},
   "source": [
    "For this part you'll need to download a geoJSON of Los Angeles and manually remove the ZIP Codes for the islands if they exist. You can try here: https://data.lacounty.gov/Geospatial/ZIP-Codes/65v5-jw9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563a8f4-be27-4ce6-8f40-a86422fca52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('la_zips_no_islands.json') as f:\n",
    "    la_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e6cf7-031a-4d2d-9a97-8bc774d991be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geojson = alt.InlineData(values=la_data, format=alt.DataFormat(property='features',type='json')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988dc7c-f7bc-453f-a2d9-9f1fe5646a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "la_zips = [x['properties']['zipcode'] for x in data_geojson['values']['features']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52417f-520b-4fa8-8da8-4dcdaa15d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('sale_max') < 10_000_000) # filter out garbage\n",
    "    .filter(pl.col('sale_max')/pl.col('sale_min') < 4) # filter out outliers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c1990-7f8d-42c1-aad5-72abe5d01605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chi_sq part of this notebook is kind of experimental\n",
    "\n",
    "def chi_sq(ys, xs_ord, popt):\n",
    "    s = 0\n",
    "    L = len(ys)\n",
    "    for i in range(L):\n",
    "        s += (ys[i] - func(xs_ord[i]-730_000, *popt)*1000)**2/L\n",
    "\n",
    "    return s\n",
    "\n",
    "def get_zip_code_app_rate(data, zip_code):\n",
    "    \n",
    "    data_specific_to_zip = data.filter(pl.col('zip5') == zip_code)\n",
    "    \n",
    "    if not data_specific_to_zip: return np.nan\n",
    "\n",
    "    if len(data_specific_to_zip) < 250: return np.nan\n",
    "\n",
    "    xs = [np.datetime64(t) for t in data_specific_to_zip['sale_date']]\n",
    "    xs_ord = np.asarray([t.toordinal() for t in data_specific_to_zip['sale_date']])\n",
    "    ys = np.asarray(data_specific_to_zip['sale_price'])\n",
    "\n",
    "    xs_tmp = xs_ord - 730_000\n",
    "    xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "    ys_tmp = ys/1000\n",
    "\n",
    "    p0 = [350, 0.0001]\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "    except (RuntimeError or TypeError):\n",
    "        return np.nan\n",
    "    \n",
    "    if np.sqrt(chi_sq(ys, xs_ord, popt)) > 900_000: return np.nan\n",
    "\n",
    "    return round(popt[1]*100*365, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a4a46-d9b4-4804-885e-5271bcb5f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "app_rates = {}\n",
    "for i in tqdm(la_zips):\n",
    "    zip_code = str(i)\n",
    "    app_rates[zip_code] = get_zip_code_app_rate(data, zip_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b92ac-9693-47e9-b52b-83d41042caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_app_data = pd.DataFrame({'zipcode': app_rates.keys(), 'rate': app_rates.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11c8f6-a4e0-4009-97f6-5dc9ee7074cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basemap = (\n",
    "    alt.Chart(data_geojson).mark_geoshape(\n",
    "        fill='black',\n",
    "        # stroke='gray',\n",
    "    )\n",
    "    .properties(width = 600, height = 600)\n",
    "    .project('mercator')\n",
    ")\n",
    "\n",
    "colors = (\n",
    "    alt.Chart(data_geojson).mark_geoshape(\n",
    "        # fill='gray',\n",
    "        # stroke='gray',\n",
    "    )\n",
    "    .encode(\n",
    "        color = alt.Color('rate:Q', legend=alt.Legend(title=\"home appreciation rate (%)\"))\n",
    "        \n",
    "    )\n",
    "    .transform_lookup(\n",
    "        lookup = 'properties.zipcode',\n",
    "        from_ = alt.LookupData(data = zip_app_data, key = 'zipcode', fields = ['rate'])\n",
    "    )\n",
    "    .properties(width = 600, height = 600)\n",
    "    .project('mercator')\n",
    ")\n",
    "\n",
    "basemap + colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fed614-2698-4e13-b889-cc6cf4020b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 400_000\n",
    "min_median = 100_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('zip5') == '90018')\n",
    "    .filter(pl.col('times_sold') > 1)\n",
    "    .filter(pl.col('sale_max') < 30_000_000)\n",
    "    .filter(pl.col('sale_max')/pl.col('sale_min') < 4)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .2,  alpha = 1)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [300, 0.0001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "ax.plot([np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt],\n",
    "        [1000*func(x-730_000, *popt) for x in xs_plt], \n",
    "        color = 'red', \n",
    "        label = f'appreciation approx. {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "\n",
    "\n",
    "s = 0\n",
    "for i in range(len(ys)):\n",
    "    s += (ys[i] - func(xs_ord[i]-730_000, *popt)*1000)**2/len(ys)\n",
    "\n",
    "print(np.sqrt(s))\n",
    "\n",
    "yticks = {\n",
    "    0: '$0',\n",
    "    # 100_000: '$100k',\n",
    "    250_000: '$250k',\n",
    "    500_000: '$500k',\n",
    "    # 700_000: '$600k',\n",
    "    1_000_000: '$1M',\n",
    "    2_000_000: '$2M',\n",
    "    # 5_000_000: '$5M',\n",
    "    # 10_000_000: '$10M',\n",
    "}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, ZIP code = 90032')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "\n",
    "plt.savefig('images/appreciation_90032.png', facecolor = 'white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865044e-8764-412e-ba3c-9aa1edc7b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import calendar\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "max_median = 400_000\n",
    "min_median = 100_000\n",
    "\n",
    "data = (\n",
    "    predata\n",
    "    .filter((pl.col('sale_year') > 2010) & (pl.col('sale_year') <= 2021))\n",
    "    .filter(pl.col('zip5') == '90015')\n",
    "    # .filter(pl.col('times_sold') > 1)\n",
    "    .filter(pl.col('sale_max') < 10_000_000)\n",
    "    .filter(pl.col('sale_max')/pl.col('sale_min') < 4)\n",
    ")\n",
    "\n",
    "xs = [np.datetime64(t) for t in data['sale_date']]\n",
    "# ordinals are needed to create a fit line\n",
    "xs_ord = np.asarray([t.toordinal() for t in data['sale_date']])\n",
    "ys = np.asarray(data['sale_price'])\n",
    "\n",
    "ax.scatter(xs, ys, s = .2,  alpha = 1)\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "xs_tmp = xs_ord - 730_000\n",
    "xs_plt = list(range(min(xs_tmp + 730_000), max(xs_tmp + 730_000) + 1))\n",
    "ys_tmp = ys/1000\n",
    "\n",
    "p0 = [300, 0.0001]\n",
    "\n",
    "popt, pcov = curve_fit(func, xs_tmp, ys_tmp, p0)\n",
    "\n",
    "xs_plt_md = [np.datetime64(datetime.fromordinal(int(a))) for a in xs_plt]\n",
    "ys_plt_md = [1000*func(x-730_000, *popt) for x in xs_plt]\n",
    "\n",
    "s = 0\n",
    "for i in range(len(ys)):\n",
    "    s += (ys[i] - func(xs_ord[i], *popt)*1000)**2\n",
    "\n",
    "print(s)\n",
    "\n",
    "ax.plot(xs_plt_md,\n",
    "        ys_plt_md,\n",
    "        color = 'red', \n",
    "        label = f'appreciation approx. {round(popt[1]*100*365, 1)}%/year')\n",
    "\n",
    "yticks = {\n",
    "    0: '$0',\n",
    "    # 100_000: '$100k',\n",
    "    250_000: '$250k',\n",
    "    500_000: '$500k',\n",
    "    # 700_000: '$600k',\n",
    "    1_000_000: '$1M',\n",
    "    2_000_000: '$2M',\n",
    "    5_000_000: '$5M',\n",
    "    # 10_000_000: '$10M',\n",
    "}\n",
    "\n",
    "ax.set_yticks([*yticks.keys()], [*yticks.values()])\n",
    "\n",
    "ax.set_title(f'Rising property values from 2010 to 2020, ZIP code = 90404')\n",
    "ax.set_ylabel('sale price [USD]')\n",
    "ax.set_xlabel('sale date')\n",
    "\n",
    "ax.set_xlim([min(xs), max(xs)])\n",
    "ax.set_ylim([min(yticks.keys()), max(yticks.keys())])\n",
    "\n",
    "ax.title.set_size(15)\n",
    "ax.xaxis.label.set_size(15)\n",
    "ax.yaxis.label.set_size(15)\n",
    "ax.xaxis.set_tick_paddrams(size = 10, labelsize = 15)\n",
    "ax.yaxis.set_tick_params(size = 10, labelsize = 15)\n",
    "\n",
    "ax.legend(markerscale = 10, loc = 'upper left', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
